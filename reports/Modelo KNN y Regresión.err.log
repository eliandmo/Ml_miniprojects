Traceback (most recent call last):
  File "C:\Users\Elian\miniconda3\Lib\site-packages\jupyter_core\utils\__init__.py", line 154, in wrapped
    asyncio.get_running_loop()
    ~~~~~~~~~~~~~~~~~~~~~~~~^^
RuntimeError: no running event loop

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Elian\miniconda3\Lib\site-packages\jupyter_cache\executors\utils.py", line 58, in single_nb_execution
    executenb(
    ~~~~~~~~~^
        nb,
        ^^^
    ...<4 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "C:\Users\Elian\miniconda3\Lib\site-packages\nbclient\client.py", line 1319, in execute
    return NotebookClient(nb=nb, resources=resources, km=km, **kwargs).execute()
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\Elian\miniconda3\Lib\site-packages\jupyter_core\utils\__init__.py", line 158, in wrapped
    return loop.run_until_complete(inner)
           ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^
  File "C:\Users\Elian\miniconda3\Lib\asyncio\base_events.py", line 725, in run_until_complete
    return future.result()
           ~~~~~~~~~~~~~^^
  File "C:\Users\Elian\miniconda3\Lib\site-packages\nbclient\client.py", line 709, in async_execute
    await self.async_execute_cell(
        cell, index, execution_count=self.code_cells_executed + 1
    )
  File "C:\Users\Elian\miniconda3\Lib\site-packages\nbclient\client.py", line 1062, in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
  File "C:\Users\Elian\miniconda3\Lib\site-packages\nbclient\client.py", line 918, in _check_raise_for_error
    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
------------------
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Definici√≥n de columnas por tipo
categorical_features = [
    'Marital Status', 'Application mode', 'Course', 
    'Daytime/evening attendance', 'Previous qualification',
    'Nacionality', "Mother's qualification", "Father's qualification",
    "Mother's occupation", "Father's occupation", 'Displaced',
    'Educational special needs', 'Debtor', 'Tuition fees up to date',
    'Gender', 'Scholarship holder', 'International'
]

numeric_features = [
    'Application order', 'Previous qualification (grade)',
    'Admission grade', 'Age at enrollment',
    'Curricular units 1st sem (credited)',
    'Curricular units 1st sem (enrolled)',
    'Curricular units 1st sem (evaluations)',
    'Curricular units 1st sem (approved)',
    'Curricular units 1st sem (grade)',
    'Curricular units 1st sem (without evaluations)',
    'Curricular units 2nd sem (credited)',
    'Curricular units 2nd sem (enrolled)',
    'Curricular units 2nd sem (evaluations)',
    'Curricular units 2nd sem (approved)',
    'Curricular units 2nd sem (grade)',
    'Curricular units 2nd sem (without evaluations)',
    'Unemployment rate', 'Inflation rate', 'GDP'
]

# Transformers para cada tipo de variable
numeric_transformer = Pipeline(steps=[
    ('scaler', MinMaxScaler())  # Normalizaci√≥n MinMax para variables num√©ricas
])

categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))  # OneHot para categ√≥ricas
])

# Combinar transformers en un ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ],
    remainder='drop'  # Ignorar cualquier otra columna no especificada
)

# Pipeline completo (puedes a√±adir el estimador al final)
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor)
    # ('classifier', ...)  # Aqu√≠ ir√≠a tu modelo KNN u otro
])

# Definir X e y
X = df.drop(['Target', 'Abandono'], axis=1)  # Eliminamos las columnas target original y nueva
y = df['Abandono']  # Usamos nuestra nueva variable binaria

# Divisi√≥n estratificada (para mantener proporci√≥n de clases)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y)

# Aplicar preprocesamiento
X_train_prep = preprocessor.fit_transform(X_train)
X_test_prep = preprocessor.transform(X_test)

# Divisi√≥n para regresi√≥n (mismos splits para consistencia)
_, _, y_reg_train, y_reg_test = train_test_split(
    X, y_reg, test_size=0.3, random_state=42)

print(f"Tama√±o del conjunto de entrenamiento: {X_train.shape[0]} muestras")
print(f"Tama√±o del conjunto de prueba: {X_test.shape[0]} muestras")
------------------


[31m---------------------------------------------------------------------------[39m
[31mNameError[39m                                 Traceback (most recent call last)
[36mCell[39m[36m [39m[32mIn[8][39m[32m, line 71[39m
[32m     67[39m X_test_prep = preprocessor.transform(X_test)
[32m     69[39m [38;5;66;03m# Divisi√≥n para regresi√≥n (mismos splits para consistencia)[39;00m
[32m     70[39m _, _, y_reg_train, y_reg_test = train_test_split(
[32m---> [39m[32m71[39m     X, [43my_reg[49m, test_size=[32m0.3[39m, random_state=[32m42[39m)
[32m     73[39m [38;5;28mprint[39m([33mf[39m[33m"[39m[33mTama√±o del conjunto de entrenamiento: [39m[38;5;132;01m{[39;00mX_train.shape[[32m0[39m][38;5;132;01m}[39;00m[33m muestras[39m[33m"[39m)
[32m     74[39m [38;5;28mprint[39m([33mf[39m[33m"[39m[33mTama√±o del conjunto de prueba: [39m[38;5;132;01m{[39;00mX_test.shape[[32m0[39m][38;5;132;01m}[39;00m[33m muestras[39m[33m"[39m)

[31mNameError[39m: name 'y_reg' is not defined

